{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn.apionly as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "- load star/quasar photometry data\n",
    "- train and evaluate using different classiefiers:\n",
    "    - *sklearn.neighbors.KNeighborsClassifier*\n",
    "    - *sklearn.ensemble.RandomForestClassifier*\n",
    "    - *sklearn.linear_model.LogisticRegression*\n",
    "    - *sklearn.svm.SVC*\n",
    "    - *sklearn.tree.DecisionTreeClassifier*\n",
    "    \n",
    "- try different hyperparameters e.g. *penalty='l2'* in Log Reression, number of estimaters, depth of a tree, criterion method in RF, k in KNN, C and kernel='rbf' with gamma.\n",
    "\n",
    "Use model? to see the help of what hyperparameters are available\n",
    "\n",
    "How do these perform on the training set vs the test set? Which one is the best on the training set, which one is the best on the test set? What about KNN with k=1 or a deep DecisionTreeClassifier with max_depth=None\n",
    "\n",
    "- What works better for training? Filters, color indices or all together?\n",
    "- Try to scale your data to zero mean and unite standard deviation. Does the result change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "# Don't forget to convert pandas dataframe to numpy array data = np.array(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Is your model really good or you were just lucky? Try cross-validation, StratifiedKFold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "- load spectral lines dataset\n",
    "- run your favourite classifier and check the performace per class\n",
    "- what if you want to put stress on precise identifying of the type 4 spectral line (the rarest case).\n",
    "\n",
    "Check the weight keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('../data/spectral_lines.npz')\n",
    "X = data['spec']\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "- Use the spectral lines dataset\n",
    "- Run PCA to shrink the dimensionality of the dataset to keep 80% of the information (in the sense of variance)\n",
    "- Run a classifier of your choice on the new dataset in lower dim space\n",
    "- Compare the clasification scores between high dim dataset (n_components=all) and low dim(n_components=5)\n",
    "- For visualisation purposes transform the data to 2D and use a scatter plot to visualise your classificatin result\n",
    "\n",
    "- Then try *sklearn.manifold.TSNE* instead of PCA, both for classification and 2D visualisation\n",
    "- Plot the learning curve. What does the result mean for learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to plot the 2D scatter plot\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 4))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def rms_error(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "\n",
    "def plot_with_err(x, data, **kwargs):\n",
    "    mu, std = data.mean(1), data.std(1)\n",
    "    lines = plt.plot(x, mu, '-', **kwargs)\n",
    "    plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n",
    "                     facecolor=lines[0].get_color(), alpha=0.2)\n",
    "    \n",
    "def plot_learning_curve(clf, X, y, cv=5):\n",
    "    train_sizes = np.linspace(0.05, 1., 10)\n",
    "    N_train, val_train, val_test = learning_curve(clf, X, y, cv=cv, scoring=rms_error, shuffle=True)\n",
    "    plot_with_err(N_train, val_train, label='training scores')\n",
    "    plot_with_err(N_train, val_test, label='validation scores')\n",
    "    plt.xlabel('Training Set Size'); plt.ylabel('rms error')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "- Try to find 4 clusters in the dataset with \n",
    "- Try to enhance it with T-SNE to 2D and visualise the ground truth and predicted clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# labels can be shuffled (permutated)\n",
    "# we are hoping that majority is clustered correctly so we assing labels to be the most common true label in the group\n",
    "from scipy.stats import mode\n",
    "\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(4):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(y[mask])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = est.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='rainbow')\n",
    "plt.title('Prediceted clusters')\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')\n",
    "plt.title('True classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y, labels))\n",
    "\n",
    "plt.imshow(confusion_matrix(y, labels),\n",
    "           cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.ylabel('true')\n",
    "plt.xlabel('predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "- Use brutal force to find the best classifier hyperparamters\n",
    "- Check SVC? what the hyperparameters mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # You may use this one or DecisionTree because they are fast\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C': np.logspace(-2, 2, 5), # from 10**-2, to 10**2\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7\n",
    "- load SDSS photometric redshift dataset (color indices or magnitudes in filters)\n",
    "- create a regression model to calculate the best redshift estimator, *sklearn.ensemble.RandomForestRegressor*, *sklearn.ensemble.GradientBoostingRegressor*\n",
    "- use cross-validation to evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load('../data/sdss_photoz_colorsz.npz')\n",
    "X = data['colors']\n",
    "z = data['redshift']\n",
    "# X = data['photom']\n",
    "# z = data['redshift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rms = np.sqrt(np.mean((z_test - z_pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting result\n",
    "\n",
    "axis_lim = np.array([-0.01, 0.8])\n",
    "plt.scatter(z_test, z_pred, s=10)\n",
    "plt.plot(axis_lim, axis_lim, '--k')\n",
    "\n",
    "plt.title('Photo-z: Decision Tree Regression')\n",
    "plt.xlabel(r'$\\mathrm{z_{true}}$', fontsize=14)\n",
    "plt.ylabel(r'$\\mathrm{z_{phot}}$', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
